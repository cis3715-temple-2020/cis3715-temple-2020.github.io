{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Twitter data analysis  \n",
    "\n",
    "In this lab, we will learn how to read JSON files and how to perform exploratory analysis of twitter data. As an extra credit, you will learn how to use API to download tweets on the topic of your choice.\n",
    "\n",
    "Let us start by importing the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "import calendar\n",
    "import codecs\n",
    "import datetime\n",
    "import sys\n",
    "import gzip\n",
    "import string\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 0: Reading Tweets\n",
    "\n",
    "Tweets are saved in the form of a JSON file. Open `onetweet` file in a text editor and study how it looks. This file contains information from a single tweet. The file is written in the JSON format, which is easy for a computer to read and parse. \n",
    "\n",
    "**Question 1**. Google `JSON` and try to learn about this particular data format. Explain in one or two paragraphs what you learned.\n",
    "\n",
    "Let us read `onetweet` JSON file. We will create an object and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# tweet = json.load(open('onetweet', 'rb').decode('utf8'))\n",
    "with open('onetweet.json') as f:\n",
    "    tweet = json.load(f)\n",
    "pprint(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each tweet is stored in a dictionary. Although there are more than 20 different keys in the dictionary, we introduce the most important fields which will be used in the rest of this lab.\n",
    "<ul>\n",
    "<li>'_id': shows the unique id of this tweet.</li>\n",
    "\n",
    "<li>'coordinates': shows the location from which the tweet was posted. The field might be null if the tweet contains no location data, or it could contain bounding box information, place information, or GPS coordinates in the form of (longitude, latitude). </li>\n",
    "\n",
    "<li>'created_at': shows the time the tweet has been created.</li>\n",
    "\n",
    "<li>'text': shows the text of the tweet.</li>\n",
    "\n",
    "<li>'user': contains multiple dictionaries describing the user, including the name of this user, the number of followers, the number of friends...</li>\n",
    "</ul>\n",
    "\n",
    "**Question 2**. Explain 3 more keys that you find interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we will load around 4,000 tweets sent from New York City region during Sandy Hurricane from a JSON file `myNYC.json`. As part of this process, we will extract each tweet's post time and create a time series of the number of tweets in each hour during the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath='smallNYC.json'\n",
    "localTweetList = []\n",
    "globalTweetCounter = 0\n",
    "frequencyMap = {}\n",
    "timeFormat = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "reader = codecs.getreader(\"utf-8\")\n",
    "for line in open(filePath, 'rb'):\n",
    "    # Try to read tweet JSON into object\n",
    "    tweetObj = None\n",
    "    tweetObj = json.loads(reader.decode(line)[0])\n",
    "    \n",
    "    # Try to extract the time of the tweet\n",
    "    currentTime=dateutil.parser.parse(tweetObj['created_at'])\n",
    "    currentTime = currentTime.replace(second=0)\n",
    "    currentTime = currentTime.replace(minute=0)\n",
    "\n",
    "    # print(currentTime)\n",
    "    # Increment tweet count\n",
    "    globalTweetCounter += 1\n",
    "    \n",
    "    # If our frequency map already has this time, use it, otherwise add\n",
    "    if currentTime in frequencyMap.keys():\n",
    "        timeMap = frequencyMap[currentTime]\n",
    "        timeMap[\"count\"] += 1\n",
    "        timeMap[\"list\"].append(tweetObj)\n",
    "    else:\n",
    "        frequencyMap[currentTime] = {\"count\":1, \"list\":[tweetObj]}\n",
    "\n",
    "# Fill in any gaps\n",
    "times = sorted(frequencyMap.keys())\n",
    "firstTime = times[0]\n",
    "lastTime = times[-1]\n",
    "thisTime = firstTime\n",
    "\n",
    "#timeIntervalStep = datetime.timedelta(0, 60)    # Time step in seconds\n",
    "timeIntervalStep = datetime.timedelta(hours=1)\n",
    "while ( thisTime <= lastTime ):\n",
    "    if ( thisTime not in frequencyMap.keys() ):\n",
    "        frequencyMap[thisTime] = {\"count\":0, \"list\":[]}\n",
    "        \n",
    "    thisTime = thisTime + timeIntervalStep\n",
    "\n",
    "print (\"Processed Tweet Count:\", globalTweetCounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Simple Frequency Analysis\n",
    "\n",
    "In this section, we will cover a few simple analysis techniques for EDA of the available twitter data.\n",
    "\n",
    "- Twitter Timeline\n",
    "- Top Twitter Users\n",
    "- Twitter API\n",
    "- Posting Frequency Distribution\n",
    "- Popular Hashtags\n",
    "- Simple Event Detection\n",
    "- Language Distributions\n",
    "\n",
    "### Twitter Timeline \n",
    "\n",
    "To build a timeline of Twitter usage, we can simply plot the number of tweets posted per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5,10.5)\n",
    "\n",
    "plt.title(\"Tweet Frequency\")\n",
    "\n",
    "# Sort the times into an array for future use\n",
    "sortedTimes = sorted(frequencyMap.keys())\n",
    "\n",
    "# What time span do these tweets cover?\n",
    "print (\"Time Frame:\", sortedTimes[0], sortedTimes[-1])\n",
    "\n",
    "# Get a count of tweets per minute\n",
    "postFreqList = [frequencyMap[x][\"count\"] for x in sortedTimes]\n",
    "\n",
    "# We'll have ticks every thirty minutes (much more clutters the graph)\n",
    "smallerXTicks = range(0, len(sortedTimes), 6)\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "# Plot the post frequency\n",
    "ax.plot(range(len(frequencyMap)), [x if x > 0 else 0 for x in postFreqList], color=\"blue\", label=\"Posts\")\n",
    "ax.grid(b=True, which=u'major')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**. When is the tweeting activity the largest? Find a wikipedia article about the Sandy Hurricane to understand the timeline of events surrounding it. Discuss if you see a correlation with the tweet frequencies. Select some tweets from different time points and see if the messages are correlated with the events on the ground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Twitter Users\n",
    "\n",
    "The following piece of code reveals the users that produced the most tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create maps for holding counts and tweets for each user\n",
    "globalUserCounter = {}\n",
    "globalUserMap = {}\n",
    "\n",
    "# Iterate through the time stamps\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    # For each tweet, pull the screen name and add it to the list\n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        user = tweet[\"user\"][\"screen_name\"]\n",
    "        \n",
    "        if user not in globalUserCounter:\n",
    "            globalUserCounter[user] = 1\n",
    "            globalUserMap[user] = [tweet]\n",
    "        else:\n",
    "            globalUserCounter[user] += 1\n",
    "            globalUserMap[user].append(tweet)\n",
    "\n",
    "print (\"Unique Users:\", len(globalUserCounter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedUsers = sorted(globalUserCounter, key=globalUserCounter.get, reverse=True)\n",
    "print (\"Top Ten Most Prolific Users:\")\n",
    "for u in sortedUsers[:10]:\n",
    "    print (u, globalUserCounter[u], \"\\n\\t\", \"Random Tweet:\", globalUserMap[u][0][\"text\"], \"\\n----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**. Find and print the top 10 users with the most friends. Find and print the top 10 users with the most followers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Postings\n",
    "\n",
    "It appears a few users were posting to Twitter a lot. But how often did most Twitter users tweet during this time? We can build a histogram to see this distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "    \n",
    "# the histogram of the data\n",
    "plt.hist(\n",
    "    [globalUserCounter[x] for x in globalUserCounter], \n",
    "    bins=100, \n",
    "    normed=0, \n",
    "    alpha=0.75,\n",
    "    label=\"Counts\",\n",
    "    log=True)\n",
    "\n",
    "plt.xlabel('Number of Tweets')\n",
    "plt.ylabel('Counts')\n",
    "plt.title(\"Histogram of Frequency\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**. Study 3 users with the most posts. What were they tweeting about?\n",
    "\n",
    "### Average Number of Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPostCount = np.mean([globalUserCounter[x] for x in globalUserCounter])\n",
    "print(\"Average Number of Posts:\", avgPostCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Hashtags\n",
    "\n",
    "Hashtags give us a quick way to view the conversation and see what people are discussing. Getting the most popular hashtags is just as easy as getting the most prolific users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A map for hashtag counts\n",
    "hashtagCounter = {}\n",
    "\n",
    "# For each minute, pull the list of hashtags and add to the counter\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        hashtagList = tweet[\"entities\"][\"hashtags\"]\n",
    "        \n",
    "        for hashtagObj in hashtagList:\n",
    "            \n",
    "            # We lowercase the hashtag to avoid duplicates (e.g., #MikeBrown vs. #mikebrown)\n",
    "            hashtagString = hashtagObj[\"text\"].lower()\n",
    "            \n",
    "            if ( hashtagString not in hashtagCounter ):\n",
    "                hashtagCounter[hashtagString] = 1\n",
    "            else:\n",
    "                hashtagCounter[hashtagString] += 1\n",
    "\n",
    "print (\"Unique Hashtags:\", len(hashtagCounter.keys()))\n",
    "sortedHashtags = sorted(hashtagCounter, key=hashtagCounter.get, reverse=True)\n",
    "print (\"Top Twenty Hashtags:\")\n",
    "for ht in sortedHashtags[:20]:\n",
    "    print (\"\\t\", \"#\" + ht, hashtagCounter[ht])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Detection w/ Keyword Frequency\n",
    "\n",
    "Twitter is good for breaking news. When an impactful event occurs, we often see a spike on Twitter of the usage of a related keyword. Some examples are below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What keywords are we interested in?\n",
    "targetKeywords = [\"obama\", \"nyc\"]\n",
    "# targetKeywords.append(\"lowery\")\n",
    "# targetKeywords.append(\"reilly\")\n",
    "targetKeywords.append(\"sandy\")\n",
    "\n",
    "# Build an empty map for each keyword we are seaching for\n",
    "targetCounts = {x:[] for x in targetKeywords}\n",
    "totalCount = []\n",
    "\n",
    "# For each minute, pull the tweet text and search for the keywords we want\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    # Temporary counter for this minute\n",
    "    localTargetCounts = {x:0 for x in targetKeywords}\n",
    "    localTotalCount = 0\n",
    "    \n",
    "    for tweetObj in timeObj[\"list\"]:\n",
    "        tweetString = tweetObj[\"text\"].lower()\n",
    "\n",
    "        localTotalCount += 1\n",
    "        \n",
    "        # Add to the counter if the target keyword is in this tweet\n",
    "        for keyword in targetKeywords:\n",
    "            if keyword in tweetString:\n",
    "                localTargetCounts[keyword] += 1\n",
    "                \n",
    "    # Add the counts for this minute to the main counter\n",
    "    totalCount.append(localTotalCount)\n",
    "    for keyword in targetKeywords:\n",
    "        targetCounts[keyword].append(localTargetCounts[keyword])\n",
    "        \n",
    "# Now plot the total frequency and frequency of each keyword\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5,10.5)\n",
    "\n",
    "plt.title(\"Tweet Frequency\")\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "ax.plot(range(len(frequencyMap)), totalCount, label=\"Total\")\n",
    "\n",
    "\n",
    "for keyword in targetKeywords:\n",
    "    ax.plot(range(len(frequencyMap)), targetCounts[keyword], label=keyword)\n",
    "ax.legend()\n",
    "ax.grid(b=True, which=u'major')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**. Study some example codes with 2 different hashtags. Explain how are those messages different. \n",
    "\n",
    "**Question 7**. Which among the top 20 hashtags are related to Sandy. What are the most popular non-Sandy hashtags? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Distribution\n",
    "\n",
    "The following code gives an insight into the languages used for the tweets in your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A map for counting each language\n",
    "languageCounter = {}\n",
    "\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        lang = tweet[\"lang\"]\n",
    "        \n",
    "        if lang not in languageCounter:\n",
    "            languageCounter[lang] = 1\n",
    "        else:\n",
    "            languageCounter[lang] += 1\n",
    "            \n",
    "languages = sorted(languageCounter.keys(), key=languageCounter.get, reverse=True)\n",
    "\n",
    "for l in languages:\n",
    "    print(l, languageCounter[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "    \n",
    "# the histogram of the data\n",
    "plt.bar(\n",
    "    np.arange(len(languages)),\n",
    "    [languageCounter[x] for x in languages],\n",
    "    log=True)\n",
    "\n",
    "plt.xticks(np.arange(len(languages)) + 0.5, languages)\n",
    "plt.xlabel('Languages')\n",
    "plt.ylabel('Counts (Log)')\n",
    "plt.title(\"Language Frequency\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quesiton 8**. Now that you have experience in extracting different types of information from twitter data, perform your own EDA. Produce a 1-page report providing some interesting insights about the Sandy twitter data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: Classification\n",
    "\n",
    "In this part of the lab, we will work on a binary classificaiton problem. In particular, we would like to see how easy it is to discriminate between tweets with hashtag #sandy and the rest of them. To do it, we will first create labels: positive tweets will be those with hashtag #sandy and negative those without the hashtag. Then, we will create a bag-of-words vector out of each tweet, but we will exclude word \"#sandy\". To produce the bag-of-words representation we will use the `CountVectorizer` functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs, json\n",
    "\n",
    "\"\"\"\n",
    "Step 1. Create sentences and labels from json file. \n",
    "\"\"\"\n",
    "filePath='smallNYC.json'\n",
    "sents = []\n",
    "reader = codecs.getreader(\"utf-8\")\n",
    "for line in open(filePath, 'rb'):\n",
    "    # Try to read tweet JSON into object\n",
    "    tweetObj = None\n",
    "    tweetObj = json.loads(reader.decode(line)[0])\n",
    "    sents.append(tweetObj['text'])\n",
    "    \n",
    "\"\"\"\n",
    "Step 2.1. Get label. If sent contains '#sandy', label=1; otherwise, label=0\n",
    "\"\"\"\n",
    "labels = np.array(['#sandy' in sent.split() for sent in sents], dtype='int')\n",
    "print(pd.Series(labels).value_counts())\n",
    "\"\"\"\n",
    "Step 2.2. Represent the data into Bag-of-words features, i.e, each sentence is a \n",
    "            vector of word counts. \n",
    "          a). Only select words with frequency >= 5\n",
    "          b). Remove label words '#sandy'\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=5, stop_words = ['#sandy'])\n",
    "features = vectorizer.fit_transform(sents)\n",
    "\n",
    "\n",
    "print('#sandy' in vectorizer.get_feature_names())\n",
    "print(features.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quesiton 9**. Given the data set of labeled tweets, you have to train a classification model and check the accuracy. Remember to first split the data into training and test. You should explore kNN classification, decision tree classification, Random Forest classification.\n",
    "\n",
    "**Question 10**. Train the so-called *Logistic regression classifier* and check its accuracy. Compare with the results from *Question 9*. The following lines of code will be useful to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3. Divide data into train and test \n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42) \n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('LogisticRegression', time.time() - start_time))\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print('Test Perf ACC: %.3f, AUC: %.3f' %(accuracy, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
